\title{KNIME - Konstanz Information Miner}
% status: 0
% chapter: TBD

\title{Big Data for Edge Computing}


\author{Harshad Pitkar}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Indiana University}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{hpitkar@iu.edu}



% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Harshad Pitkar}


\begin{abstract}

KNIME also known as Konstanz information miner is an open source
analytics platform that provides a drag and drop GUI interface for
components involved in building a data pipeline such as loading data
from disparate sources, agrregation, data exploration, statistical
functions, machine learning algorithms and finally visualization.
KNIME support reading from a variety of popular formats including csv,
excel, json and xml to name a few. It supports reading data from a
wide range of databases and supports accessing them using jdbc or a
product specific connector for Microsoft SQL server, MySQL and others.
KNIME supports a veriety of machine learning algorithms for
regression, classification, PCA and so on. It also provides deep
learning framework through Keras which enables users to use a variety
of deep learning frameworks such as TensorFlow, cognitive tool
kit~\cite{hid-sp18-517-kdl}. A workflow is a collection of nodes where
a node is a single unit or a step that does processing such as reading
from files, connecting to a database. Nodes are connected to other
nodes in a wprkflow. The GUI interface that provides drag and drop
interface and facilitates the building of a workflow is called a
workbench.

\end{abstract}

\keywords{517, KNIME, Workflow, Text, mining}


\maketitle

\section{Introduction}

The world of information technology is experiencing Data deluge
sometimes also refered as the Data Tsunami wherein almost every
organization is trying to make sense of all the data that is being
collected. Almost every company has invested or is planning invest
into big data projects. Analytics and predictive analytics has been
the key to this process. Naturally, we see an array of tools, open
source to commercial being introduced frequently to support the
analytics requirements. In most cases, the analytics process
progresses through a series of steps from data loading, wrangling,
cleaning to make it ready for performing analysis and building machine
learning models to bring out the value from data.  The entire process
requires multiple tools, programming as well as manual steps.  KNIME
analytics platform works at all these steps and provides an
abstraction layer to all the underlying complexities. KNIME provides a
GUI interface that is user friendly and takes out the complexities of
writing complex code to perform tasks and build models. Subsequent
sections will provide more insight into KNIME tool its features and
how they can be leveraged to complete an analytics project.

\section{Architecture}
KNIME software is bundled with multiple components such as KNIME
analytics platform which forms the core of the tool, KNIME server that
provides the scalability. KNIME extensions enable the integration with
other open source projects including Apache hadoop. Finally it also
support community and partner extensions which makes it favourable for
all environments~\cite{hid-sp18-517-ksw}.  See
Figure~\ref{f:knimearch}~\cite{hid-sp18-517-ksw}.

\section{KNIME Analytics platform}
KNIME analytics is the core of the product and is built on eclipse and
hence you would notice the interface is somewhat similar to
eclipse. It's grahical and user friendly interface enables faster data
science cycles and makes it easier to learn as well. It can be
installed on Windows, MAC and linux and comes with many ready to run
examples~\cite{hid-sp18-517-kap}.

A node is a smallest processing unit in KNIME and it supposed to
perform a specific task for example loading of a csv file. Similarly
collection of a nodes sequenced in order makes a workflow. Nodes
within a workflow may be connected to each other and output of one
node could be an input to the other depending on the
sequence~\cite{hid-sp18-517-kintro}.

The GUI interface is divided into multiple sections, mainly workbench,
workflow coach, explorer and the node repository. Workbench is the
area on which you actually drag and drop nodes. For example, if I have
to read a csv file and load it into a database, I would create a node
by dragging the read node type on the workbench followed by a
read-write node of the database type and link them, naturally,
additional configuration of columns and so on would be needed.  An
interesting feature of KNIME is Workflow coach which is an inbuilt
recommender system that provides recommendations on what type of node
to use as the next step, these recommendations are built based on the
community usage of those type of nodes~\cite{hid-sp18-517-ch1sec1}.


\section{KNIME Server}
In a large scale implementation, scalability can be achieved by
deploying a KNIME server. KNIME server scales the platform from an
individual to a group or a team of data scientists and facilitates
collaboration, deployment and management
functionalities~\cite{hid-sp18-517-server}. KNIME server is
available in three editions small, medium and large based on number of
users and features. Small edition is suitable for a smaller teams and
has no support available except for forum access whereas medium and
large offer higher number of users and more features including the
ability to deploy workflows as REST API~\cite{hid-sp18-517-editions}.

\section{Extensions}
KNIME supports both open source and commercial extensions that help 
in integrating with the analytics platform. This enables the users to integrate
R or python within the workflow. Similarly Apache Hadoop, Spark and other 
Big Data open source projects can be integrated with KNIME through respective 
estensions. A good Big Data integration example is being able to import or 
export data from HDFS or performing analytics using Hive, Impala that are
setup as KNIME nodes in a workflow~\cite{hid-sp18-517-ksw}. 

Big data extensions are included in the platform and include those for
Apache Hadoop, Spark that enable reading and writing from HDFS, Hive
as well as Impala. This extension as such provides a graphical
interface for big data applications.

\section{Analytics}
KNIME Analytics: KNIME provides deep learning capability through KNIME
analytics platform that enables users to create, train and execute
deep learning models. Also allows integration with Keras that enables
users to use deep learnign frameworks such as Tensorflow and
others~\cite{hid-sp18-517-dl}. In the meachine learning space it
supports all majority of predictive modeling algorithms for
regression, classigication, Neural networks, Naive Bayes and most of
the tree based models~\cite{hid-sp18-517-pml}.

It also supports transferring data from and to H2O
~\cite{hid-sp18-517-h20} which is a popular open source machine
learning platform~\cite{hid-sp18-517-knimeh20}.

\section{REST support}

If your project involves fetching data through a REST API then KNIME
Workflow allows you to configure nodes that can get data from these
available REST services~\cite{hid-sp18-517-knimeapi}.  KNIME server
also makes it possible to create a service that can be called using
post and get methods.  Again all this can be done using KNIME workflow
and nodes on drag and drop graphical user
interface~\cite{hid-sp18-517-knimerest}.


\section{Conclusion}

In todays world of analytics which is heavily focussed on scripting to
perform various tasks in Data science cycle, KNIME provides not only
an easy to use Graphical user interface but supports an array of
integrations which makes it a good choice. It has options for
scalability and the user friendly GUI take out the need to know
scripting and every command.  The popularity and succes of KNIME is
evident by the fact that Gartner has placed KNIE as leader in Data
Science and Machine learning platform from 2013 to
2018~\cite{hid-sp18-517-dsml}.

\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski for his
  support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 



